import math
from typing import Optional

import numpy as np
import torch
from sklearn.cluster import KMeans
from torch.optim import Optimizer


class ClusterCoupledAdam(Optimizer):
    """
    Cluster-Coupled AdamW optimizer.
    - ì¼ë¶€ param groupì— ëŒ€í•´:
      * ì£¼ê¸°ì ìœ¼ë¡œ K-meansë¡œ row ë‹¨ìœ„ í´ëŸ¬ìŠ¤í„°ë§
      * í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  gradientë¥¼ ê³„ì‚° í›„ í˜¼í•©
      * í˜¼í•©ëœ gradientë¡œ AdamW ì—…ë°ì´íŠ¸
    - param group ì˜µì…˜:
      * clustered: True/False
      * num_clusters: K (elbow methodë¡œ 2~K ì¤‘ ì„ íƒ)
      * alpha: í´ëŸ¬ìŠ¤í„° ê³µìœ  ê°•ë„ (0~1)
      * recluster_interval: ì¬í´ëŸ¬ìŠ¤í„°ë§ ì£¼ê¸° (step ê¸°ì¤€)
      * cluster_source: "param" / "grad" / "ema_grad"
      * cluster_beta: EMA ê³„ìˆ˜ (cluster_source="ema_grad"ì¼ ë•Œ ì‚¬ìš©)
      * cluster_start_step: ëª‡ ë²ˆì§¸ stepë¶€í„° í´ëŸ¬ìŠ¤í„°ë§ì„ ì¼¤ì§€ (burn-in ìš©, ê¸°ë³¸ 0)
    """

    def __init__(
        self,
        params,
        lr=1e-3,
        betas=(0.9, 0.999),
        eps=1e-8,
        weight_decay=0.0,
        alpha=0.5,
        num_clusters=8,
        recluster_interval=100,
        cluster_source="param",
        cluster_beta=0.9,
        cluster_start_step=0,
        min_cluster_rows=2,
    ):
        defaults = dict(
            lr=lr,
            betas=betas,
            eps=eps,
            weight_decay=weight_decay,
            alpha=alpha,
            num_clusters=num_clusters,
            recluster_interval=recluster_interval,
            cluster_source=cluster_source,
            cluster_beta=cluster_beta,
            cluster_start_step=cluster_start_step,
            min_cluster_rows=min_cluster_rows,
        )
        super().__init__(params, defaults)

    def _find_best_k_elbow(self, w2d, k_min=2, k_max=16):
        """
        elbow methodë¡œ ìµœì  Kë¥¼ ê³ ë¥´ëŠ” í•¨ìˆ˜.
        - w2d: (num_rows, dim) numpy array
        - k_min, k_max: íƒìƒ‰í•  K ë²”ìœ„
        ë°˜í™˜: best_k (int)
        """
        Ks = list(range(k_min, k_max + 1))
        inertias = []

        for K in Ks:
            kmeans = KMeans(n_clusters=K, random_state=42)
            kmeans.fit(w2d)
            inertias.append(kmeans.inertia_)

        x1, y1 = Ks[0], inertias[0]
        x2, y2 = Ks[-1], inertias[-1]

        distances = []
        for x, y in zip(Ks, inertias):
            num = abs((y2 - y1) * x - (x2 - x1) * y + x2 * y1 - y2 * x1)
            den = math.sqrt((y2 - y1) ** 2 + (x2 - x1) ** 2)
            distances.append(num / (den + 1e-12))

        best_idx = int(np.argmax(distances))
        best_k = Ks[best_idx]
        return best_k

    @torch.no_grad()
    def _update_clusters(
        self,
        base_tensor,
        state,
        num_clusters,
        recluster_interval,
        min_cluster_rows,
    ):
        """
        í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„° í…ì„œì— ëŒ€í•´ K-means í´ëŸ¬ìŠ¤í„°ë§ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜.

        - base_tensor: í´ëŸ¬ìŠ¤í„°ë§ ê¸°ì¤€ í…ì„œ (ì˜ˆ: p.data, grad, EMA-grad)
        - state: ì´ íŒŒë¼ë¯¸í„°ì— í•´ë‹¹í•˜ëŠ” optimizer state dict
        - num_clusters: elbow íƒìƒ‰ ì‹œ ì‚¬ìš©í•  ìµœëŒ€ K (k_max ì—­í• )
        - recluster_interval: ëª‡ stepë§ˆë‹¤ í´ëŸ¬ìŠ¤í„°ë§ì„ ë‹¤ì‹œ í• ì§€
        """
        step = state.get("cluster_step", 0)
        assignments = state.get("assignments", None)

        # ì•„ì§ í´ëŸ¬ìŠ¤í„°ë§ ì•ˆ í–ˆê±°ë‚˜, ì¼ì • stepë§ˆë‹¤ ì¬í´ëŸ¬ìŠ¤í„°ë§í•  ë•Œë§Œ ìƒˆë¡œ ê³„ì‚°
        if assignments is None or step % recluster_interval == 0:
            w2d = base_tensor.detach().view(base_tensor.shape[0], -1).cpu().numpy()

            k_max = min(num_clusters, w2d.shape[0])
            if w2d.shape[0] < min_cluster_rows:
                assignments = torch.zeros(
                    w2d.shape[0], dtype=torch.long, device=base_tensor.device
                )
                state["assignments"] = assignments
                state["cluster_step"] = step + 1
                return assignments

            if k_max < 2:
                assignments = torch.zeros(
                    w2d.shape[0], dtype=torch.long, device=base_tensor.device
                )
            else:
                best_K = state.get("best_K", None)
                if best_K is None:
                    best_K = self._find_best_k_elbow(w2d, k_min=2, k_max=k_max)
                    state["best_K"] = best_K

                    print(
                        f"[ClusterCoupledAdam] step={step} | "
                        f"rows={w2d.shape[0]} | k_max={k_max} | best_K(elbow)={best_K}"
                    )
                else:
                    best_K = min(best_K, k_max)

                if best_K < 2:
                    assignments = torch.zeros(
                        w2d.shape[0], dtype=torch.long, device=base_tensor.device
                    )
                else:
                    kmeans = KMeans(n_clusters=best_K, random_state=42)
                    labels = kmeans.fit_predict(w2d)
                    assignments = torch.from_numpy(labels).long().to(base_tensor.device)

            state["assignments"] = assignments

        state["cluster_step"] = step + 1

        return state["assignments"]

    @torch.no_grad()
    def step(self, closure=None):
        """
        Optimizerì˜ í•œ step ìˆ˜í–‰ í•¨ìˆ˜.
        - ê° param groupê³¼ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´:
          1) weight decay ì ìš© (AdamW ë°©ì‹)
          2) (clustered=True & burn-in ì§€ë‚˜ë©´) gradientë¥¼ í´ëŸ¬ìŠ¤í„° í‰ê· ê³¼ ì„ì–´ì„œ gÌƒ ê³„ì‚°
          3) Adam ëª¨ë©˜íŠ¸ ì—…ë°ì´íŠ¸ ë° íŒŒë¼ë¯¸í„° ê°±ì‹ 
        """
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            lr = group["lr"]
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            weight_decay = group["weight_decay"]

            # cluster ê´€ë ¨ ì˜µì…˜
            clustered = group.get("clustered", False)
            alpha = group.get("alpha", 0.5)
            num_clusters = group.get("num_clusters", 8)
            recluster_interval = group.get("recluster_interval", 100)
            min_cluster_rows = group.get("min_cluster_rows", 2)

            cluster_source = group.get("cluster_source", "param")   # "param" / "grad" / "ema_grad"
            cluster_beta   = group.get("cluster_beta", 0.9)         # EMA ê³„ìˆ˜ (0.9 ~ 0.99 ì •ë„)
            cluster_start_step = group.get("cluster_start_step", 0) # burn-in ëë‚˜ëŠ” step

            for p in group["params"]:
                if p.grad is None:
                    continue
                grad = p.grad.data
                if grad.is_sparse:
                    raise RuntimeError("ClusterCoupledAdam does not support sparse gradients.")

                state = self.state[p]
                # state ì´ˆê¸°í™”
                if len(state) == 0:
                    state["step"] = 0
                    state["exp_avg"] = torch.zeros_like(
                        p, memory_format=torch.preserve_format
                    )
                    state["exp_avg_sq"] = torch.zeros_like(
                        p, memory_format=torch.preserve_format
                    )
                    if clustered:
                        state["cluster_step"] = 0
                        state["assignments"] = None
                        if cluster_source == "ema_grad":
                            state["ema_grad"] = torch.zeros_like(
                                p, memory_format=torch.preserve_format
                            )

                exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
                state["step"] += 1
                current_step = state["step"]  ### ğŸ”¹ í˜„ì¬ step

                # 1) AdamW ìŠ¤íƒ€ì¼ weight decay
                if weight_decay != 0.0:
                    grad = grad.add(p.data, alpha=weight_decay)

                # 2) Cluster-Coupled gradient í˜¼í•© ë¶€ë¶„
                #    â†’ burn-in êµ¬ê°„(current_step < cluster_start_step)ì—ì„œëŠ” **í˜¼í•© X**
                if clustered and current_step >= cluster_start_step:   ### ğŸ”¹ ì¡°ê±´ ì¶”ê°€
                    if p.data.dim() != 2:
                        raise ValueError(
                            "Clustered param must be a 2D tensor (e.g., [num_embeddings, dim])."
                        )

                    # EMA-grad ì—…ë°ì´íŠ¸ (cluster_source == "ema_grad"ì¼ ë•Œë§Œ)
                    if cluster_source == "ema_grad":
                        ema = state.get("ema_grad", None)
                        if ema is None:
                            ema = torch.zeros_like(grad)
                            state["ema_grad"] = ema
                        ema.mul_(cluster_beta).add_(grad, alpha=1.0 - cluster_beta)
                        base_tensor = ema
                    elif cluster_source == "param":
                        base_tensor = p.data
                    elif cluster_source == "grad":
                        base_tensor = grad
                    else:
                        raise ValueError(f"Unknown cluster_source: {cluster_source}")

                    # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
                    assignments = self._update_clusters(
                        base_tensor,
                        state,
                        num_clusters,
                        recluster_interval,
                        min_cluster_rows,
                    )

                    if assignments.dim() != 1 or assignments.size(0) != p.shape[0]:
                        raise ValueError("Cluster assignments must be 1D (num_rows,).")

                    g2d = grad.view(p.shape[0], -1)
                    K = int(assignments.max().item() + 1)

                    gc = torch.zeros(K, g2d.size(1), device=grad.device)
                    idx_expanded = assignments.view(-1, 1).expand(-1, g2d.size(1))
                    gc.scatter_add_(0, idx_expanded, g2d)

                    counts = torch.bincount(
                        assignments, minlength=K
                    ).float().to(grad.device).unsqueeze(1)
                    counts = counts.clamp_min(1.0)
                    gc = gc / counts

                    mixed = (1.0 - alpha) * g2d + alpha * gc[assignments]
                    grad = mixed.view_as(grad)
                # else: burn-in êµ¬ê°„ì—ì„œëŠ” gradë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í´ëŸ¬ìŠ¤í„°ë§ ì•ˆ í•¨)

                # 3) Adam ì—…ë°ì´íŠ¸ (í‘œì¤€ Adam)
                beta1_t, beta2_t = beta1, beta2
                exp_avg.mul_(beta1_t).add_(grad, alpha=1 - beta1_t)
                exp_avg_sq.mul_(beta2_t).addcmul_(grad, grad, value=1 - beta2_t)

                denom = exp_avg_sq.sqrt().add_(eps)

                bias_correction1 = 1 - beta1_t ** current_step
                bias_correction2 = 1 - beta2_t ** current_step
                step_size = lr * math.sqrt(bias_correction2) / bias_correction1

                p.data.addcdiv_(exp_avg, denom, value=-step_size)

        return loss

class ClusterCoupledSGD(Optimizer):
    """
    Cluster-Coupled SGD
    - í´ëŸ¬ìŠ¤í„°ë§ëœ íŒŒë¼ë¯¸í„° ê·¸ë£¹ì— ëŒ€í•´:
        gÌƒ_i = (1 - Î±) g_i + Î± * g_cluster(i)
    - ì´í›„ SGD ì—…ë°ì´íŠ¸: p <- p - lr * gÌƒ
    """

    def __init__(
        self,
        params,
        lr=1e-2,
        momentum=0.0,
        weight_decay=0.0,
    ):
        defaults = dict(
            lr=lr,
            momentum=momentum,
            weight_decay=weight_decay,
        )
        super().__init__(params, defaults)

    @torch.no_grad()
    def _update_clusters(self, p, state, num_clusters, recluster_interval):
        step = state.get("cluster_step", 0)
        assignments = state.get("assignments", None)

        if assignments is None or step % recluster_interval == 0:
            w2d = p.data.detach().view(p.shape[0], -1).cpu().numpy()

            K = min(num_clusters, w2d.shape[0])
            if K < 2:
                assignments = torch.zeros(w2d.shape[0], dtype=torch.long, device=p.device)
            else:
                km = KMeans(n_clusters=K, random_state=42)
                labels = km.fit_predict(w2d)
                assignments = torch.from_numpy(labels).long().to(p.device)

            state["assignments"] = assignments

        state["cluster_step"] = step + 1
        return state["assignments"]

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            weight_decay = group["weight_decay"]

            clustered = group.get("clustered", False)
            alpha = group.get("alpha", 0.5)
            num_clusters = group.get("num_clusters", 8)
            recluster_interval = group.get("recluster_interval", 100)

            for p in group["params"]:
                if p.grad is None:
                    continue

                grad = p.grad.data
                state = self.state[p]

                if len(state) == 0:
                    state["momentum_buffer"] = torch.zeros_like(p)

                    if clustered:
                        state["cluster_step"] = 0
                        state["assignments"] = None

                # weight decay ì ìš©
                if weight_decay != 0:
                    grad = grad.add(p.data, alpha=weight_decay)

                # í´ëŸ¬ìŠ¤í„° gradient í‰ê·  ì„ê¸°
                if clustered:
                    if p.data.dim() != 2:
                        raise ValueError("Clustered param must be 2D")

                    assignments = self._update_clusters(
                        p, state, num_clusters, recluster_interval
                    )

                    g2d = grad.view(assignments.numel(), -1)
                    K = int(assignments.max().item()) + 1

                    gc = torch.zeros(K, g2d.size(1), device=grad.device)
                    idx_expanded = assignments.view(-1, 1).expand(-1, g2d.size(1))
                    gc.scatter_add_(0, idx_expanded, g2d)

                    counts = torch.bincount(assignments, minlength=K).float().to(p.device)
                    counts = counts.clamp_min(1).unsqueeze(1)
                    gc = gc / counts

                    mixed = (1 - alpha) * g2d + alpha * gc[assignments]
                    grad = mixed.view_as(grad)

                # SGD with momentum
                buf = state["momentum_buffer"]
                buf.mul_(momentum).add_(grad)

                p.data.add_(buf, alpha=-lr)

        return loss

class ClusterCoupledRMSProp(Optimizer):
    """
    Cluster-Coupled RMSProp

    gÌƒ_i = (1 - Î±) g_i + Î± * g_cluster(i)
    p â† p - lr * gÌƒ_i / (sqrt(E[gÌƒ_i^2]) + eps)
    """

    def __init__(
        self,
        params,
        lr=1e-3,
        alpha_cluster=0.5,
        num_clusters=16,
        recluster_interval=100,
        rho=0.9,
        eps=1e-8,
        weight_decay=0.0,
    ):
        defaults = dict(
            lr=lr,
            alpha_cluster=alpha_cluster,
            num_clusters=num_clusters,
            recluster_interval=recluster_interval,
            rho=rho,
            eps=eps,
            weight_decay=weight_decay,
        )
        super().__init__(params, defaults)

    @torch.no_grad()
    def _update_clusters(self, p, state, num_clusters, recluster_interval):
        step = state.get("cluster_step", 0)
        assignments = state.get("assignments", None)

        if assignments is None or step % recluster_interval == 0:
            w2d = p.data.detach().view(p.shape[0], -1).cpu().numpy()

            K = min(num_clusters, w2d.shape[0])
            if K < 2:
                assignments = torch.zeros(w2d.shape[0], dtype=torch.long, device=p.device)
            else:
                km = KMeans(n_clusters=K, random_state=42)
                labels = km.fit_predict(w2d)
                assignments = torch.from_numpy(labels).long().to(p.device)

            state["assignments"] = assignments

        state["cluster_step"] = step + 1
        return state["assignments"]

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            lr = group["lr"]
            rho = group["rho"]
            eps = group["eps"]
            weight_decay = group["weight_decay"]

            alpha_c = group["alpha_cluster"]
            num_clusters = group["num_clusters"]
            recluster_interval = group["recluster_interval"]

            clustered = group.get("clustered", False)

            for p in group["params"]:
                if p.grad is None:
                    continue

                grad = p.grad.data
                state = self.state[p]

                if len(state) == 0:
                    state["square_avg"] = torch.zeros_like(p)
                    if clustered:
                        state["cluster_step"] = 0
                        state["assignments"] = None

                # weight decay
                if weight_decay != 0:
                    grad = grad.add(p.data, alpha=weight_decay)

                # ---- Cluster Mixing ----
                if clustered:
                    if p.data.dim() != 2:
                        raise ValueError("Clustered param must be 2D")

                    assignments = self._update_clusters(
                        p, state, num_clusters, recluster_interval
                    )

                    g2d = grad.view(assignments.numel(), -1)

                    K = int(assignments.max().item()) + 1
                    gc = torch.zeros(K, g2d.size(1), device=p.device)

                    idx_expanded = assignments.view(-1, 1).expand(-1, g2d.size(1))
                    gc.scatter_add_(0, idx_expanded, g2d)

                    counts = torch.bincount(assignments, minlength=K).float().to(p.device)
                    counts = counts.clamp_min(1).unsqueeze(1)

                    gc = gc / counts
                    mixed = (1 - alpha_c) * g2d + alpha_c * gc[assignments]
                    grad = mixed.view_as(grad)

                # ---- RMSProp ì—…ë°ì´íŠ¸ ----
                square_avg = state["square_avg"]
                square_avg.mul_(rho).addcmul_(grad, grad, value=1 - rho)

                avg = grad / (square_avg.sqrt() + eps)

                p.data.add_(avg, alpha=-lr)

        return loss

class Lamb(Optimizer):
    def __init__(self,
                 params,
                 lr=1e-3,
                 betas=(0.9, 0.999),
                 eps=1e-6,
                 weight_decay=0,
                 clamp_value=10,
                 debias=True):
        defaults = dict(
            lr=lr,
            betas=betas,
            eps=eps,
            weight_decay=weight_decay,
            clamp_value=clamp_value,
            debias=debias
        )
        super(Lamb, self).__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            lr = group['lr']
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            clamp_value = group['clamp_value']
            debias = group['debias']

            for p in group['params']:
                if p.grad is None:
                    continue
                grad = p.grad

                if grad.is_sparse:
                    raise RuntimeError("LAMB does not support sparse gradients.")

                state = self.state[p]

                # State Initialization
                if len(state) == 0:
                    state["step"] = 0
                    state["exp_avg"] = torch.zeros_like(p)
                    state["exp_avg_sq"] = torch.zeros_like(p)

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]

                state["step"] += 1
                step = state["step"]

                # Adam moment update
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)

                # Adam bias correction
                if debias:
                    bias_correction1 = 1 - beta1 ** step
                    bias_correction2 = 1 - beta2 ** step
                    m = exp_avg / bias_correction1
                    v = exp_avg_sq / bias_correction2
                else:
                    m, v = exp_avg, exp_avg_sq

                adam_step = m / (v.sqrt() + eps)

                # Decoupled weight decay
                if wd != 0:
                    adam_step = adam_step + wd * p

                # Trust ratio
                w_norm = p.norm(p=2)
                g_norm = adam_step.norm(p=2)

                if w_norm.item() > 0 and g_norm.item() > 0:
                    trust_ratio = w_norm / g_norm
                else:
                    trust_ratio = 1.0

                trust_ratio = min(trust_ratio.item(), clamp_value)

                p.add_(adam_step, alpha=-lr * trust_ratio) # Parameter update

        return loss
